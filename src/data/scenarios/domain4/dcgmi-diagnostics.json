{
  "id": "domain4-dcgmi-diag",
  "title": "DCGM Diagnostics and Health Monitoring",
  "domain": "domain4",
  "difficulty": "intermediate",
  "description": "Learn how to use NVIDIA Data Center GPU Manager (DCGM) for comprehensive GPU diagnostics, health monitoring, and validation. DCGM is essential for detecting GPU issues before they impact production workloads.",
  "learningObjectives": [
    "Understand DCGM architecture and capabilities",
    "Run multi-level DCGM diagnostics",
    "Monitor GPU health metrics with DCGM",
    "Interpret diagnostic results and identify issues"
  ],
  "faults": [],
  "initialClusterState": {},
  "steps": [
    {
      "id": "step1",
      "title": "Verify DCGM Installation",
      "description": "Check that DCGM and dcgmi CLI tool are installed and the DCGM daemon is running.",
      "objectives": [
        "Verify dcgmi is installed",
        "Check DCGM version",
        "Verify nv-hostengine is running",
        "Test basic dcgmi connectivity"
      ],
      "expectedCommands": [
        "dcgmi --version",
        "dcgmi discovery -l"
      ],
      "validationRules": [
        {
          "type": "command-executed",
          "description": "Must check DCGM installation",
          "expectedCommands": [
            "dcgmi --version",
            "dcgmi -v"
          ],
          "requireAllCommands": true
        }
      ],
      "hints": [
        "Use 'dcgmi --version' to check installation",
        "DCGM consists of nv-hostengine daemon and dcgmi client",
        "Use 'dcgmi discovery -l' to list discovered GPUs",
        "If dcgmi fails to connect, check if nv-hostengine is running",
        "DCGM can run in embedded mode (no daemon) or standalone mode"
      ],
      "estimatedDuration": 5,
      "documentationLinks": [
        {
          "title": "DCGM Documentation",
          "url": "https://docs.nvidia.com/datacenter/dcgm/"
        }
      ]
    },
    {
      "id": "step2",
      "title": "Discover and List GPUs",
      "description": "Use DCGM to discover all GPUs in the system and display their properties.",
      "objectives": [
        "Discover all GPUs with DCGM",
        "List GPU IDs and UUIDs",
        "View basic GPU properties",
        "Verify GPU count matches physical installation"
      ],
      "expectedCommands": [
        "dcgmi discovery -l",
        "dcgmi discovery -c"
      ],
      "validationRules": [
        {
          "type": "command-executed",
          "description": "Must list GPUs with DCGM",
          "expectedCommands": [
            "dcgmi discovery -l",
            "dcgmi discovery"
          ],
          "requireAllCommands": true
        }
      ],
      "hints": [
        "Use 'dcgmi discovery -l' to list all GPUs",
        "Use 'dcgmi discovery -c' to show compute instances (for MIG)",
        "Each GPU has a GPU ID (0, 1, 2...) and UUID",
        "UUID is globally unique and persistent across reboots",
        "Count of discovered GPUs should match system configuration"
      ],
      "estimatedDuration": 5,
      "documentationLinks": [
        {
          "title": "DCGM Discovery",
          "url": "https://docs.nvidia.com/datacenter/dcgm/"
        }
      ]
    },
    {
      "id": "step3",
      "title": "Run Level 1 Diagnostics (Quick)",
      "description": "Execute DCGM Level 1 diagnostics which perform quick software and deployment checks without stressing the GPU.",
      "objectives": [
        "Run dcgmi diag at level 1",
        "Understand Level 1 test coverage",
        "Interpret diagnostic results",
        "Identify any failed tests"
      ],
      "expectedCommands": [
        "dcgmi diag -r 1",
        "dcgmi diag --run 1"
      ],
      "validationRules": [
        {
          "type": "command-executed",
          "description": "Must run Level 1 diagnostics",
          "expectedCommands": [
            "dcgmi diag -r 1",
            "dcgmi diag --run 1"
          ],
          "requireAllCommands": true
        }
      ],
      "hints": [
        "Use 'dcgmi diag -r 1' or 'dcgmi diag --run 1' to execute Level 1 diagnostics",
        "Both short form (-r) and long form (--run) options work",
        "Level 1 tests: Software, permissions, libraries, basic GPU properties",
        "Level 1 is non-intrusive and completes in seconds",
        "Results show Pass/Fail/Skip for each test",
        "Common Level 1 tests: Deployment, Driver Loaded, CUDA Runtime, Permissions",
        "All tests should show 'Pass' for a healthy system"
      ],
      "estimatedDuration": 7,
      "documentationLinks": [
        {
          "title": "DCGM Diagnostics Levels",
          "url": "https://docs.nvidia.com/datacenter/dcgm/"
        }
      ]
    },
    {
      "id": "step4",
      "title": "Run Level 2 Diagnostics (Medium)",
      "description": "Execute DCGM Level 2 diagnostics which include short hardware stress tests to verify GPU functionality.",
      "objectives": [
        "Run dcgmi diag at level 2",
        "Understand Level 2 test coverage",
        "Monitor test progress",
        "Review detailed test results"
      ],
      "expectedCommands": [
        "dcgmi diag -r 2",
        "dcgmi diag --run 2"
      ],
      "validationRules": [
        {
          "type": "command-executed",
          "description": "Must run Level 2 diagnostics with stress tests",
          "expectedCommands": [
            "dcgmi diag -r 2",
            "dcgmi diag --run 2"
          ],
          "requireAllCommands": true
        }
      ],
      "hints": [
        "Use 'dcgmi diag -r 2' or 'dcgmi diag --run 2' for Level 2 diagnostics",
        "Both short form (-r) and long form (--run) options are valid",
        "Level 2 includes Level 1 tests + short stress tests",
        "Tests: Memory bandwidth, SM stress, targeted stress",
        "Takes 1-2 minutes per GPU",
        "Level 2 detects most common GPU hardware issues",
        "Watch for any warnings or errors in output"
      ],
      "estimatedDuration": 8,
      "documentationLinks": [
        {
          "title": "DCGM Level 2 Tests",
          "url": "https://docs.nvidia.com/datacenter/dcgm/"
        }
      ]
    },
    {
      "id": "step5",
      "title": "Understand Level 3 Diagnostics",
      "description": "Learn about DCGM Level 3 diagnostics which perform extended stress tests to thoroughly validate GPU hardware.",
      "objectives": [
        "Understand Level 3 test scope",
        "Learn when to use Level 3",
        "Understand time requirements",
        "Know Level 3 use cases"
      ],
      "expectedCommands": [
        "dcgmi diag --help",
        "dcgmi diag -r 3"
      ],
      "validationRules": [
        {
          "type": "command-executed",
          "description": "Must understand Level 3 diagnostics",
          "expectedCommands": [
            "dcgmi diag --help",
            "dcgmi diag -r 3"
          ],
          "requireAllCommands": true
        }
      ],
      "hints": [
        "Level 3 runs extended tests (10-15 minutes per GPU)",
        "Tests: Long SM stress, memory stress, PCIe bandwidth, NVLink",
        "Level 3 is most comprehensive but time-consuming",
        "Use Level 3 for: New hardware acceptance, after repairs, periodic validation",
        "WARNING: Level 3 puts significant load on GPUs and power/cooling",
        "All GPUs should be idle before running Level 3"
      ],
      "estimatedDuration": 7,
      "documentationLinks": [
        {
          "title": "DCGM Level 3 Tests",
          "url": "https://docs.nvidia.com/datacenter/dcgm/"
        }
      ]
    },
    {
      "id": "step6",
      "title": "Monitor GPU Health Metrics",
      "description": "Use DCGM to continuously monitor GPU health metrics including temperature, power, utilization, and memory.",
      "objectives": [
        "Monitor real-time GPU metrics",
        "Understand key health indicators",
        "Use dcgmi dmon command",
        "Interpret metric values"
      ],
      "expectedCommands": [
        "dcgmi dmon",
        "dcgmi dmon -e"
      ],
      "validationRules": [
        {
          "type": "command-executed",
          "description": "Must monitor GPU metrics",
          "expectedCommands": [
            "dcgmi dmon"
          ],
          "requireAllCommands": true
        }
      ],
      "hints": [
        "Use 'dcgmi dmon' to monitor GPUs in real-time",
        "Use 'dcgmi dmon -e' to show specific fields",
        "Key metrics: GPU utilization, memory utilization, temperature, power",
        "Update frequency can be controlled with -i flag",
        "DCGM can also export metrics to Prometheus/Grafana",
        "Use Ctrl+C to stop monitoring"
      ],
      "estimatedDuration": 7,
      "documentationLinks": [
        {
          "title": "DCGM Monitoring",
          "url": "https://docs.nvidia.com/datacenter/dcgm/"
        }
      ]
    },
    {
      "id": "step7",
      "title": "Check GPU Telemetry and Errors",
      "description": "Query GPU telemetry data and check for any ECC errors or XID errors reported by DCGM.",
      "objectives": [
        "Query GPU statistics",
        "Check ECC error counts",
        "Review any XID errors",
        "Understand error significance"
      ],
      "expectedCommands": [
        "dcgmi stats -v",
        "dcgmi health -c"
      ],
      "validationRules": [
        {
          "type": "command-executed",
          "description": "Must check GPU statistics and health",
          "expectedCommands": [
            "dcgmi stats",
            "dcgmi health"
          ],
          "requireAllCommands": true
        }
      ],
      "hints": [
        "Use 'dcgmi stats -v' for detailed GPU statistics",
        "Use 'dcgmi health -c' to check health status",
        "Zero ECC errors is ideal; non-zero requires investigation",
        "Single-bit ECC errors are corrected but indicate potential issues",
        "Double-bit ECC errors are uncorrectable and serious",
        "XID errors indicate GPU hardware/driver errors",
        "Use 'nvidia-smi -q' for more detailed ECC information"
      ],
      "estimatedDuration": 6,
      "documentationLinks": [
        {
          "title": "GPU Telemetry",
          "url": "https://docs.nvidia.com/datacenter/dcgm/"
        }
      ]
    }
  ],
  "successCriteria": [
    "Verified DCGM installation and connectivity",
    "Discovered all GPUs in the system",
    "Successfully ran Level 1 diagnostics",
    "Successfully ran Level 2 diagnostics",
    "Understood Level 3 diagnostic scope",
    "Monitored real-time GPU health metrics",
    "Checked telemetry and ECC error counts"
  ],
  "estimatedTime": 45,
  "prerequisites": [
    "domain1-driver-install"
  ],
  "tags": [
    "dcgm",
    "diagnostics",
    "health-monitoring",
    "validation",
    "intermediate"
  ]
}