{
  "id": "domain4-cluster-health",
  "title": "Comprehensive Cluster Health Validation",
  "domain": "domain4",
  "difficulty": "advanced",
  "description": "Learn how to perform comprehensive health validation of an entire DGX cluster. This scenario combines multiple tools and checks to ensure the cluster is production-ready.",
  "learningObjectives": [
    "Perform systematic cluster-wide health checks",
    "Validate hardware, firmware, and software consistency",
    "Verify networking and storage infrastructure",
    "Create a cluster health report",
    "Understand acceptance testing procedures"
  ],
  "faults": [],
  "initialClusterState": {},
  "steps": [
    {
      "id": "step1",
      "title": "Inventory All Cluster Nodes",
      "description": "Create a comprehensive inventory of all nodes in the cluster including hardware specifications, firmware versions, and software versions.",
      "objectives": [
        "List all nodes in the cluster",
        "Document hardware configuration per node",
        "Record firmware and BIOS versions",
        "Verify consistency across nodes"
      ],
      "expectedCommands": [
        "bcm-node list",
        "sinfo -Nel"
      ],
      "validationRules": [
        {
          "type": "command-executed",
          "description": "Must inventory cluster nodes",
          "expectedCommands": [
            "bcm-node list",
            "sinfo -Nel",
            "scontrol show nodes"
          ],
          "requireAllCommands": true
        }
      ],
      "hints": [
        "Use 'bcm-node list' to see BCM node inventory (BCM-specific)",
        "Alternative: Use 'pdsh -a hostname' or 'clush -a hostname' for non-BCM clusters",
        "Use 'sinfo -Nel' to see Slurm node list (if using Slurm)",
        "Alternative: Use 'kubectl get nodes' for Kubernetes clusters",
        "Check consistency: All nodes should have same GPU count and type",
        "Verify BIOS versions match across nodes",
        "Document any exceptions or variations",
        "Create inventory spreadsheet or JSON file"
      ],
      "estimatedDuration": 10,
      "documentationLinks": [
        {
          "title": "Cluster Inventory Best Practices",
          "url": "https://docs.nvidia.com/dgx/"
        }
      ]
    },
    {
      "id": "step2",
      "title": "Validate All GPUs Across Cluster",
      "description": "Verify that all GPUs across all nodes are detected, healthy, and have consistent firmware/driver versions.",
      "objectives": [
        "Count total GPUs across all nodes",
        "Verify all GPUs are detected and operational",
        "Check driver version consistency",
        "Verify GPU firmware consistency"
      ],
      "expectedCommands": [
        "nvidia-smi",
        "nvidia-smi --query-gpu=gpu_name,driver_version,vbios_version --format=csv"
      ],
      "validationRules": [
        {
          "type": "command-executed",
          "description": "Must validate all GPUs",
          "expectedCommands": [
            "nvidia-smi",
            "nvidia-smi --query-gpu"
          ],
          "requireAllCommands": true
        }
      ],
      "hints": [
        "Run nvidia-smi on each node (use Slurm or parallel ssh)",
        "Example: srun --nodes=all nvidia-smi --query-gpu=name,count --format=csv",
        "Verify driver version is identical across all nodes",
        "Check vBIOS versions match for same GPU models",
        "Total GPU count should match cluster specification",
        "Zero ECC errors and no Xid errors on any GPU"
      ],
      "estimatedDuration": 10,
      "documentationLinks": [
        {
          "title": "GPU Fleet Management",
          "url": "https://docs.nvidia.com/datacenter/tesla/"
        }
      ]
    },
    {
      "id": "step3",
      "title": "Run DCGM Diagnostics on All Nodes",
      "description": "Execute DCGM Level 2 diagnostics on all GPUs across the entire cluster to identify any hardware issues.",
      "objectives": [
        "Run DCGM diagnostics cluster-wide",
        "Collect diagnostic results from all nodes",
        "Identify any failed diagnostics",
        "Document any issues found"
      ],
      "expectedCommands": [
        "dcgmi diag -r 2"
      ],
      "validationRules": [
        {
          "type": "command-executed",
          "description": "Must run cluster-wide diagnostics",
          "expectedCommands": [
            "dcgmi diag -r 2",
            "dcgmi diag --mode 2"
          ],
          "requireAllCommands": true
        }
      ],
      "hints": [
        "Use Slurm to run dcgmi on all nodes in parallel",
        "Example: srun --nodes=all dcgmi diag -r 2",
        "Level 2 provides good balance of coverage vs time",
        "Save diagnostic output to files for analysis",
        "All tests should show 'Pass' for healthy cluster",
        "Any 'Fail' requires investigation before production",
        "Consider Level 3 diagnostics for new hardware acceptance"
      ],
      "estimatedDuration": 15,
      "documentationLinks": [
        {
          "title": "Fleet Diagnostics",
          "url": "https://docs.nvidia.com/datacenter/dcgm/"
        }
      ]
    },
    {
      "id": "step4",
      "title": "Verify Network Infrastructure",
      "description": "Validate that all networking components (InfiniBand, Ethernet management, NVLink) are operational and properly configured.",
      "objectives": [
        "Verify InfiniBand connectivity",
        "Check InfiniBand link speeds",
        "Test management network connectivity",
        "Validate NVLink topology on all nodes"
      ],
      "expectedCommands": [
        "ibstat",
        "nvidia-smi topo -m"
      ],
      "validationRules": [
        {
          "type": "command-executed",
          "description": "Must verify network infrastructure",
          "expectedCommands": [
            "ibstat",
            "nvidia-smi topo -m",
            "ping"
          ],
          "requireAllCommands": true
        }
      ],
      "hints": [
        "Use 'ibstat' to check InfiniBand HCA status and link state",
        "Verify all IB ports show 'Active' state",
        "Check link speed matches spec (e.g., HDR: 200 Gb/s)",
        "Test node-to-node connectivity with ib_write_bw",
        "Verify management network with ping and SSH access",
        "Use 'nvidia-smi topo -m' to verify NVLink on each node",
        "All NVLink connections should show 'NV18' (H100) or 'NV12' (A100)"
      ],
      "estimatedDuration": 12,
      "documentationLinks": [
        {
          "title": "Network Validation",
          "url": "https://docs.nvidia.com/networking/"
        }
      ]
    },
    {
      "id": "step5",
      "title": "Validate Shared Storage",
      "description": "Verify that all shared storage filesystems are mounted and accessible from all nodes with proper permissions and performance.",
      "objectives": [
        "Verify NFS/parallel filesystem mounts",
        "Check storage accessibility from all nodes",
        "Test read/write permissions",
        "Validate storage performance"
      ],
      "expectedCommands": [
        "df -h",
        "mount | grep nfs"
      ],
      "validationRules": [
        {
          "type": "command-executed",
          "description": "Must validate shared storage",
          "expectedCommands": [
            "df -h",
            "mount | grep nfs",
            "ls /cm_shared"
          ],
          "requireAllCommands": true
        }
      ],
      "hints": [
        "Common mounts: /home, /cm_shared, /data",
        "Use 'df -h' to see all mounted filesystems",
        "Verify mount points exist on all nodes",
        "Test write access: touch /cm_shared/test_file",
        "Check NFS version and mount options (vers=4, rw, hard)",
        "Test storage performance with dd or fio",
        "Ensure user home directories are accessible",
        "Verify no stale NFS handles"
      ],
      "estimatedDuration": 10,
      "documentationLinks": [
        {
          "title": "Storage Validation",
          "url": "https://docs.nvidia.com/dgx/"
        }
      ]
    },
    {
      "id": "step6",
      "title": "Verify Workload Manager Configuration",
      "description": "Validate Slurm configuration across the cluster including partitions, GRES, and job scheduling.",
      "objectives": [
        "Verify all nodes are visible to Slurm",
        "Check GPU GRES configuration",
        "Test job submission and execution",
        "Verify accounting database"
      ],
      "expectedCommands": [
        "sinfo",
        "scontrol show partition",
        "squeue"
      ],
      "validationRules": [
        {
          "type": "command-executed",
          "description": "Must validate Slurm configuration",
          "expectedCommands": [
            "sinfo",
            "scontrol show partition",
            "squeue"
          ],
          "requireAllCommands": true
        }
      ],
      "hints": [
        "Use 'sinfo' to verify all nodes are idle/allocated (not down/drain)",
        "Use 'scontrol show nodes' to verify GPU GRES on each node",
        "Submit test job and verify it runs: srun -N1 --gres=gpu:1 nvidia-smi",
        "Check Slurm accounting: sacct -S today",
        "Verify partition configuration matches requirements",
        "Test multi-node job submission",
        "Ensure slurmctld and slurmd services running on all nodes"
      ],
      "estimatedDuration": 12,
      "documentationLinks": [
        {
          "title": "Slurm Validation",
          "url": "https://slurm.schedmd.com/"
        }
      ]
    },
    {
      "id": "step7",
      "title": "Run Multi-Node Communication Test",
      "description": "Execute a multi-node NCCL test to validate GPU communication across the entire cluster.",
      "objectives": [
        "Run cluster-wide NCCL all-reduce test",
        "Verify inter-node GPU communication",
        "Measure cluster-wide bandwidth",
        "Identify any communication bottlenecks"
      ],
      "expectedCommands": [
        "squeue",
        "sinfo"
      ],
      "validationRules": [
        {
          "type": "command-executed",
          "description": "Must verify cluster is ready for test",
          "expectedCommands": [
            "squeue",
            "sinfo"
          ],
          "requireAllCommands": true
        }
      ],
      "hints": [
        "Use Slurm to launch NCCL test across all nodes",
        "Example: srun -N 4 --ntasks-per-node=8 --gres=gpu:8 nccl-test",
        "Set NCCL_IB_DISABLE=0 for InfiniBand",
        "Set NCCL_DEBUG=INFO to see topology detection",
        "Expected bandwidth depends on cluster interconnect",
        "Look for successful completion across all ranks",
        "Any failures indicate network or GPU issues",
        "Compare bandwidth against baseline for cluster config"
      ],
      "estimatedDuration": 15,
      "documentationLinks": [
        {
          "title": "Cluster Communication Testing",
          "url": "https://docs.nvidia.com/deeplearning/nccl/"
        }
      ]
    },
    {
      "id": "step8",
      "title": "Generate Cluster Health Report",
      "description": "Compile all health check results into a comprehensive cluster health report documenting the validation results and any issues found.",
      "objectives": [
        "Summarize all health check results",
        "Document any failures or warnings",
        "Create action items for issues",
        "Sign off on cluster readiness"
      ],
      "expectedCommands": [
        "bcm validate pod",
        "sinfo"
      ],
      "validationRules": [
        {
          "type": "command-executed",
          "description": "Must generate validation summary",
          "expectedCommands": [
            "bcm validate pod",
            "sinfo"
          ],
          "requireAllCommands": true
        }
      ],
      "hints": [
        "Use 'bcm validate pod' for BCM validation summary",
        "Include: Node count, GPU count, firmware versions, diagnostic results",
        "Document network topology and bandwidth test results",
        "List any warnings or issues requiring attention",
        "Include software versions (drivers, CUDA, NCCL, frameworks)",
        "Get sign-off from stakeholders before production use",
        "Store report for compliance and future reference"
      ],
      "estimatedDuration": 16,
      "documentationLinks": [
        {
          "title": "Cluster Acceptance Testing",
          "url": "https://docs.nvidia.com/dgx/"
        }
      ]
    }
  ],
  "successCriteria": [
    "Completed comprehensive node inventory",
    "Validated all GPUs across cluster",
    "Ran DCGM diagnostics on all nodes",
    "Verified network infrastructure",
    "Validated shared storage access",
    "Verified Slurm configuration",
    "Successfully ran multi-node communication test",
    "Generated comprehensive health report"
  ],
  "estimatedTime": 100,
  "prerequisites": [
    "domain1-driver-install",
    "domain3-slurm-config",
    "domain4-dcgmi-diag",
    "domain4-nccl-test"
  ],
  "tags": [
    "cluster-health",
    "validation",
    "acceptance-testing",
    "production-readiness",
    "advanced"
  ]
}