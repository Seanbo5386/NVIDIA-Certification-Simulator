{
  "id": "domain4-nccl-test",
  "title": "NCCL Communication Testing and Validation",
  "domain": "domain4",
  "difficulty": "advanced",
  "description": "Learn how to validate multi-GPU and multi-node communication using NVIDIA Collective Communications Library (NCCL) tests. NCCL is critical for distributed training and multi-GPU performance.",
  "learningObjectives": [
    "Understand NCCL architecture and communication patterns",
    "Run NCCL all-reduce performance tests",
    "Validate NVLink bandwidth with NCCL",
    "Identify communication bottlenecks",
    "Test multi-node GPU communication"
  ],
  "faults": [],
  "initialClusterState": {},
  "steps": [
    {
      "id": "step1",
      "title": "Verify NCCL Installation",
      "description": "Check that NCCL library is installed and compatible with your CUDA version. Verify NCCL test binaries are available.",
      "objectives": [
        "Locate NCCL library installation",
        "Check NCCL version",
        "Verify NCCL tests are built",
        "Understand NCCL dependencies"
      ],
      "expectedCommands": [
        "ldconfig -p | grep libnccl",
        "ls /usr/local/nccl-tests/"
      ],
      "validationRules": [
        {
          "type": "command-executed",
          "description": "Must verify NCCL installation",
          "expectedCommands": [
            "ldconfig -p | grep libnccl",
            "ls /usr/local/nccl-tests"
          ],
          "requireAllCommands": true
        }
      ],
      "hints": [
        "NCCL library is typically at /usr/lib/x86_64-linux-gnu/libnccl.so",
        "Use 'ldconfig -p | grep libnccl' to find NCCL library",
        "NCCL tests repo: https://github.com/NVIDIA/nccl-tests",
        "Tests are typically in /usr/local/nccl-tests/build/",
        "NCCL version should match CUDA version (e.g., NCCL 2.20 for CUDA 12.4)",
        "Check NGC PyTorch/TensorFlow containers - they include NCCL"
      ],
      "estimatedDuration": 5,
      "documentationLinks": [
        {
          "title": "NCCL Documentation",
          "url": "https://docs.nvidia.com/deeplearning/nccl/"
        }
      ]
    },
    {
      "id": "step2",
      "title": "Verify GPU Topology for NCCL",
      "description": "Check GPU topology to understand NVLink connectivity which is critical for NCCL performance.",
      "objectives": [
        "Display GPU topology matrix",
        "Verify NVLink connections between GPUs",
        "Understand topology impact on NCCL"
      ],
      "expectedCommands": [
        "nvidia-smi topo -m",
        "nvidia-smi nvlink --status"
      ],
      "validationRules": [
        {
          "type": "command-executed",
          "description": "Must check GPU topology",
          "expectedCommands": [
            "nvidia-smi topo -m",
            "nvidia-smi topo --matrix"
          ],
          "requireAllCommands": true
        }
      ],
      "hints": [
        "Use 'nvidia-smi topo -m' to see GPU topology matrix",
        "NVLink connections show as 'NV#' in the matrix",
        "All GPUs should be interconnected via NVLink for best NCCL performance",
        "Use 'nvidia-smi nvlink --status' to verify all NVLink connections are active",
        "For H100 DGX, expect NV18 (900 GB/s bidirectional) between GPUs"
      ],
      "estimatedDuration": 5,
      "documentationLinks": [
        {
          "title": "GPU Topology",
          "url": "https://docs.nvidia.com/datacenter/tesla/pdf/NVIDIA-Multi-Instance-GPU-User-Guide.pdf"
        }
      ]
    },
    {
      "id": "step3",
      "title": "Understand NCCL Test Parameters",
      "description": "Learn about NCCL test parameters and what they measure before running tests.",
      "objectives": [
        "Understand algBW vs busBW metrics",
        "Learn test parameter meanings",
        "Know expected bandwidth values",
        "Understand message size impact"
      ],
      "expectedCommands": [
        "nvidia-smi nvlink --status",
        "nvidia-smi"
      ],
      "validationRules": [
        {
          "type": "command-executed",
          "description": "Must verify GPUs are ready for testing",
          "expectedCommands": [
            "nvidia-smi"
          ],
          "requireAllCommands": true
        }
      ],
      "hints": [
        "algBW = algorithm bandwidth (effective bandwidth for your operation)",
        "busBW = bus bandwidth (actual data transferred on interconnect)",
        "For all-reduce: busBW = algBW * 2 * (n-1) / n, where n = number of GPUs",
        "Message sizes: small (<1KB) = latency bound, large (>1MB) = bandwidth bound",
        "For H100 with NVLink, expect 200-300 GB/s algBW for large messages",
        "Ensure no other GPU workloads are running during tests"
      ],
      "estimatedDuration": 5,
      "documentationLinks": [
        {
          "title": "Understanding NCCL Performance",
          "url": "https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/usage/performance.html"
        }
      ]
    },
    {
      "id": "step4",
      "title": "Analyze NCCL Performance Expectations",
      "description": "Understand expected NCCL performance based on hardware topology and message sizes.",
      "objectives": [
        "Understand bandwidth scaling with message size",
        "Learn latency vs bandwidth tradeoffs",
        "Know peak performance expectations",
        "Identify performance bottlenecks"
      ],
      "expectedCommands": [
        "nvidia-smi nvlink --status",
        "nvidia-smi topo -m"
      ],
      "validationRules": [
        {
          "type": "command-executed",
          "description": "Must verify NVLink status",
          "expectedCommands": [
            "nvidia-smi nvlink --status",
            "nvidia-smi topo -m"
          ],
          "requireAllCommands": true
        }
      ],
      "hints": [
        "Output columns: size, count, type, redop, root, time, algBW, busBW",
        "Small messages (<1KB): Limited by latency",
        "Medium messages (1KB-1MB): Bandwidth increases",
        "Large messages (>1MB): Peak bandwidth achieved",
        "For 8x H100 DGX, expect algBW > 200 GB/s at 128MB",
        "busBW accounts for bidirectional traffic in all-reduce",
        "Use 'nvidia-smi nvlink --status' to verify all links are active"
      ],
      "estimatedDuration": 7,
      "documentationLinks": [
        {
          "title": "NCCL Performance Analysis",
          "url": "https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/usage/performance.html"
        }
      ]
    },
    {
      "id": "step5",
      "title": "Understand Collective Operations",
      "description": "Learn about different NCCL collective operations and their performance characteristics.",
      "objectives": [
        "Understand all-reduce operation",
        "Learn about all-gather and reduce-scatter",
        "Understand broadcast operation",
        "Compare operation performance"
      ],
      "expectedCommands": [
        "nvidia-smi"
      ],
      "validationRules": [
        {
          "type": "command-executed",
          "description": "Must verify GPUs are idle",
          "expectedCommands": [
            "nvidia-smi"
          ],
          "requireAllCommands": true
        }
      ],
      "hints": [
        "NCCL provides separate test binaries for each operation",
        "all_reduce_perf, all_gather_perf, reduce_scatter_perf, broadcast_perf",
        "All-reduce typically has highest bandwidth (optimized path)",
        "All-gather and reduce-scatter have similar performance",
        "Broadcast is typically slower (one sender, multiple receivers)",
        "All-reduce is most common in distributed training",
        "Ensure no other GPU workloads are running during tests"
      ],
      "estimatedDuration": 8,
      "documentationLinks": [
        {
          "title": "NCCL Collectives",
          "url": "https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/usage/collectives.html"
        }
      ]
    },
    {
      "id": "step6",
      "title": "Understand Multi-Node NCCL",
      "description": "Learn about multi-node NCCL requirements and configuration for testing across multiple nodes.",
      "objectives": [
        "Understand multi-node NCCL requirements",
        "Learn about network interconnects",
        "Understand GPU Direct RDMA",
        "Know MPI launcher requirements"
      ],
      "expectedCommands": [
        "ibstat",
        "nvidia-smi topo -m"
      ],
      "validationRules": [
        {
          "type": "command-executed",
          "description": "Must verify network topology",
          "expectedCommands": [
            "ibstat",
            "nvidia-smi topo -m"
          ],
          "requireAllCommands": true
        }
      ],
      "hints": [
        "Multi-node NCCL requires MPI (OpenMPI, MVAPICH2) or launcher",
        "Set NCCL_IB_DISABLE=0 to enable InfiniBand",
        "Set NCCL_NET_GDR_LEVEL=PHB for GPU Direct RDMA",
        "Use mpirun or Slurm to launch across nodes",
        "Example: mpirun -np 16 -npernode 8 ./all_reduce_perf",
        "Inter-node bandwidth will be lower than intra-node (limited by network)",
        "InfiniBand HDR: ~200 Gb/s (25 GB/s)",
        "Verify IB connectivity with 'ibstat' or 'ibv_devinfo'"
      ],
      "estimatedDuration": 10,
      "documentationLinks": [
        {
          "title": "Multi-Node NCCL",
          "url": "https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/env.html"
        }
      ]
    },
    {
      "id": "step7",
      "title": "Configure NCCL Environment Variables",
      "description": "Learn key NCCL environment variables for debugging and performance tuning.",
      "objectives": [
        "Understand NCCL_DEBUG for troubleshooting",
        "Configure NCCL network settings",
        "Optimize NCCL performance",
        "Enable NCCL logging"
      ],
      "expectedCommands": [
        "echo $NCCL_DEBUG",
        "env | grep NCCL"
      ],
      "validationRules": [
        {
          "type": "command-executed",
          "description": "Must review NCCL environment variables",
          "expectedCommands": [
            "echo $NCCL_DEBUG",
            "env | grep NCCL"
          ],
          "requireAllCommands": true
        }
      ],
      "hints": [
        "NCCL_DEBUG=INFO: Shows initialization and configuration info",
        "NCCL_DEBUG=WARN: Shows warnings (default)",
        "NCCL_DEBUG=TRACE: Verbose logging for debugging",
        "NCCL_IB_DISABLE=0: Enable InfiniBand (1 to disable)",
        "NCCL_NET_GDR_LEVEL=PHB: Enable GPU Direct RDMA",
        "NCCL_P2P_LEVEL=NVL: Force NVLink for intra-node (PHB for PCIe)",
        "NCCL_ALGO=Tree/Ring: Select communication algorithm",
        "Use NCCL_DEBUG=INFO for first test to verify topology detection"
      ],
      "estimatedDuration": 8,
      "documentationLinks": [
        {
          "title": "NCCL Environment Variables",
          "url": "https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/env.html"
        }
      ]
    },
    {
      "id": "step8",
      "title": "Troubleshoot NCCL Performance Issues",
      "description": "Learn how to identify and troubleshoot common NCCL performance problems.",
      "objectives": [
        "Identify NVLink issues affecting NCCL",
        "Detect PCIe fallback scenarios",
        "Recognize network configuration problems",
        "Use NCCL_DEBUG for troubleshooting"
      ],
      "expectedCommands": [
        "nvidia-smi nvlink --status",
        "nvidia-smi topo -m"
      ],
      "validationRules": [
        {
          "type": "command-executed",
          "description": "Must verify interconnect status",
          "expectedCommands": [
            "nvidia-smi nvlink --status",
            "nvidia-smi topo -m"
          ],
          "requireAllCommands": true
        }
      ],
      "hints": [
        "Low bandwidth? Check NVLink status with 'nvidia-smi nvlink --status'",
        "Look for 'Using network' vs 'Using NVLink' in NCCL_DEBUG=INFO output",
        "Check topology: 'nvidia-smi topo -m' should show NV# between GPUs",
        "If using PCIe instead of NVLink, check for NVLink errors",
        "Multi-node: Verify InfiniBand/network with ibstat or ping",
        "Check CPU affinity - GPUs should use local NUMA CPUs",
        "Use NCCL_DEBUG=INFO NCCL_DEBUG_SUBSYS=INIT,NET to see detailed init"
      ],
      "estimatedDuration": 10,
      "documentationLinks": [
        {
          "title": "NCCL Troubleshooting",
          "url": "https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/troubleshooting.html"
        }
      ]
    }
  ],
  "successCriteria": [
    "Verified NCCL installation and version",
    "Understood GPU topology for NCCL",
    "Learned NCCL test parameters and metrics",
    "Analyzed performance expectations",
    "Understood different collective operations",
    "Learned multi-node communication requirements",
    "Configured NCCL environment variables",
    "Understood troubleshooting techniques"
  ],
  "estimatedTime": 58,
  "prerequisites": [
    "domain1-driver-install",
    "domain2-nvlink-topo"
  ],
  "tags": [
    "nccl",
    "collective-communications",
    "multi-gpu",
    "distributed-training",
    "advanced"
  ]
}