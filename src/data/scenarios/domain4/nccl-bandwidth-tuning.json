{
  "id": "domain4-nccl-tuning",
  "title": "NCCL Bandwidth Tuning and Collective Optimization",
  "domain": "domain4",
  "difficulty": "advanced",
  "description": "Master NCCL (NVIDIA Collective Communications Library) performance tuning for multi-GPU and multi-node AI workloads. NCCL is critical for distributed deep learning, and understanding how to tune it for maximum bandwidth is essential for the NCP-AII certification.",
  "learningObjectives": [
    "Understand NCCL architecture and collectives",
    "Run and interpret NCCL benchmark tests",
    "Tune NCCL environment variables for performance",
    "Optimize for NVLink vs InfiniBand topologies",
    "Diagnose NCCL performance issues"
  ],
  "faults": [],
  "initialClusterState": {},
  "steps": [
    {
      "id": "step1",
      "title": "Understand NCCL Collectives",
      "description": "NCCL provides optimized collective operations for multi-GPU communication. Understanding the different collectives is essential for performance tuning.",
      "objectives": [
        "Learn NCCL collective operations",
        "Understand all-reduce, all-gather, broadcast",
        "Know when each collective is used"
      ],
      "expectedCommands": [
        "nccl-test --help",
        "ls /usr/local/nccl-tests/build/"
      ],
      "validationRules": [
        {
          "type": "command-executed",
          "description": "Must explore NCCL tests",
          "expectedCommands": ["nccl-test", "nccl"]
        }
      ],
      "hints": [
        "ALL-REDUCE: Sum gradients across GPUs (most common in DL)",
        "ALL-GATHER: Collect data from all GPUs to all GPUs",
        "BROADCAST: Send data from one GPU to all others",
        "REDUCE-SCATTER: Reduce and distribute results",
        "Training uses ALL-REDUCE for gradient synchronization",
        "Inference uses BROADCAST for model distribution"
      ],
      "estimatedDuration": 5,
      "documentationLinks": [
        {
          "title": "NCCL Documentation",
          "url": "https://docs.nvidia.com/deeplearning/nccl/"
        }
      ]
    },
    {
      "id": "step2",
      "title": "Run NCCL Baseline Tests",
      "description": "Run NCCL-tests to establish baseline performance for your cluster configuration.",
      "objectives": [
        "Run all_reduce_perf test",
        "Test with different message sizes",
        "Record baseline bandwidth"
      ],
      "expectedCommands": [
        "nccl-test -b 8 -e 8G -f 2 -g 8",
        "docker run --gpus all -v /dev/shm:/dev/shm nvcr.io/nvidia/pytorch:23.10-py3 python -c 'import torch; print(torch.cuda.nccl.version())'",
        "mpirun -np 8 ./all_reduce_perf -b 8 -e 1G -f 2 -g 1"
      ],
      "validationRules": [
        {
          "type": "command-executed",
          "description": "Must run NCCL baseline tests",
          "expectedCommands": ["nccl-test", "all_reduce"]
        }
      ],
      "hints": [
        "-b: Start message size, -e: End message size",
        "-f 2: Double size each iteration",
        "-g: GPUs per process (8 for single-node DGX)",
        "Expected A100 NVLink bandwidth: ~200+ GB/s",
        "Expected H100 NVLink bandwidth: ~450+ GB/s",
        "Note 'Avg' column for usable bandwidth"
      ],
      "estimatedDuration": 8,
      "documentationLinks": [
        {
          "title": "NCCL Tests",
          "url": "https://github.com/NVIDIA/nccl-tests"
        }
      ]
    },
    {
      "id": "step3",
      "title": "Analyze NVLink vs PCIe Performance",
      "description": "Compare NCCL performance over NVLink versus PCIe to verify NVLink is being utilized properly.",
      "objectives": [
        "Check NVLink topology",
        "Force PCIe-only mode for comparison",
        "Measure performance difference"
      ],
      "expectedCommands": [
        "nvidia-smi topo -m",
        "NCCL_P2P_DISABLE=1 nccl-test -b 8 -e 1G -g 8",
        "NCCL_DEBUG=INFO nccl-test -b 1M -e 1M -g 8 2>&1 | grep -i nvlink"
      ],
      "validationRules": [
        {
          "type": "command-executed",
          "description": "Must analyze transport modes",
          "expectedCommands": ["nvidia-smi topo", "NCCL_P2P_DISABLE"]
        }
      ],
      "hints": [
        "NCCL_P2P_DISABLE=1 forces PCIe (much slower)",
        "NVLink should be 10-20x faster than PCIe",
        "NCCL_DEBUG=INFO shows transport selection",
        "Look for 'P2P/NVLINK' in debug output",
        "If NCCL chooses PCIe when NVLink available = problem",
        "Check fabric manager is running for NVSwitch"
      ],
      "estimatedDuration": 8,
      "documentationLinks": [
        {
          "title": "NCCL Environment Variables",
          "url": "https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/env.html"
        }
      ]
    },
    {
      "id": "step4",
      "title": "Tune NCCL Environment Variables",
      "description": "Apply NCCL environment variable tuning for optimal performance based on your hardware configuration.",
      "objectives": [
        "Set optimal buffer sizes",
        "Configure threading model",
        "Enable performance features"
      ],
      "expectedCommands": [
        "export NCCL_SOCKET_IFNAME=ib0",
        "export NCCL_IB_HCA=mlx5",
        "NCCL_BUFFSIZE=16777216 NCCL_NTHREADS=512 nccl-test -b 8 -e 1G -g 8"
      ],
      "validationRules": [
        {
          "type": "command-executed",
          "description": "Must tune NCCL variables",
          "expectedCommands": ["NCCL_BUFFSIZE", "NCCL_NTHREADS"]
        }
      ],
      "hints": [
        "NCCL_BUFFSIZE: Buffer size per connection (default 4MB)",
        "Larger buffers (16MB) better for large messages",
        "NCCL_NTHREADS: Threads per collective (256-512)",
        "NCCL_SOCKET_IFNAME: Network interface for TCP",
        "NCCL_IB_HCA: InfiniBand HCA selection",
        "NCCL_ALGO: Force algorithm (Tree, Ring)"
      ],
      "estimatedDuration": 8,
      "documentationLinks": [
        {
          "title": "NCCL Tuning Guide",
          "url": "https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/usage/tuning.html"
        }
      ]
    },
    {
      "id": "step5",
      "title": "Multi-Node NCCL Configuration",
      "description": "Configure NCCL for multi-node communication over InfiniBand for distributed training.",
      "objectives": [
        "Verify InfiniBand connectivity",
        "Configure NCCL for IB transport",
        "Run multi-node benchmark"
      ],
      "expectedCommands": [
        "ibstat",
        "export NCCL_IB_DISABLE=0",
        "mpirun -np 16 -hostfile hosts.txt ./all_reduce_perf -b 8 -e 1G -g 8"
      ],
      "validationRules": [
        {
          "type": "command-executed",
          "description": "Must configure multi-node NCCL",
          "expectedCommands": ["ibstat", "NCCL_IB_DISABLE", "mpirun"]
        }
      ],
      "hints": [
        "NCCL_IB_DISABLE=0 enables InfiniBand (default)",
        "NCCL_IB_GID_INDEX for RoCE configurations",
        "Expected 2-node bandwidth: ~95% of single-node",
        "Use ibstat to verify IB port state=Active",
        "NCCL_NET_GDR_LEVEL for GPU Direct RDMA",
        "Match MPI ranks to GPU topology"
      ],
      "estimatedDuration": 10,
      "documentationLinks": [
        {
          "title": "NCCL Multi-Node",
          "url": "https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/mpi.html"
        }
      ]
    },
    {
      "id": "step6",
      "title": "Diagnose NCCL Performance Issues",
      "description": "Learn to diagnose common NCCL performance problems using debug output and monitoring.",
      "objectives": [
        "Enable NCCL debug logging",
        "Identify transport problems",
        "Detect bandwidth bottlenecks"
      ],
      "expectedCommands": [
        "NCCL_DEBUG=INFO NCCL_DEBUG_SUBSYS=ALL nccl-test -b 1M -e 1M -g 8 2>&1 | head -100",
        "nvidia-smi dmon -s pucvmet -d 5",
        "NCCL_DEBUG=WARN nccl-test -b 8 -e 1G -g 8"
      ],
      "validationRules": [
        {
          "type": "command-executed",
          "description": "Must diagnose NCCL issues",
          "expectedCommands": ["NCCL_DEBUG", "nvidia-smi dmon"]
        }
      ],
      "hints": [
        "NCCL_DEBUG levels: VERSION, WARN, INFO, TRACE",
        "NCCL_DEBUG_SUBSYS: INIT, COLL, P2P, NET, ALL",
        "Look for 'Using network' messages",
        "Check for fallback to slower transports",
        "Monitor GPU utilization during collectives",
        "Low bandwidth = check transport selection"
      ],
      "estimatedDuration": 8,
      "documentationLinks": [
        {
          "title": "NCCL Debugging",
          "url": "https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/usage/debugging.html"
        }
      ]
    },
    {
      "id": "step7",
      "title": "Document Optimal Configuration",
      "description": "Create a documented optimal NCCL configuration for production use.",
      "objectives": [
        "Record optimal settings",
        "Create environment module",
        "Validate final performance"
      ],
      "expectedCommands": [
        "cat > nccl_optimal.sh << 'EOF'\nexport NCCL_BUFFSIZE=16777216\nexport NCCL_NTHREADS=512\nexport NCCL_IB_HCA=mlx5\nexport NCCL_SOCKET_IFNAME=ib0\nEOF",
        "source nccl_optimal.sh && nccl-test -b 8 -e 8G -g 8"
      ],
      "validationRules": [
        {
          "type": "command-executed",
          "description": "Must document configuration",
          "expectedCommands": ["nccl_optimal", "source"]
        }
      ],
      "hints": [
        "Document: Hardware config, NCCL version, settings",
        "Create reproducible benchmark script",
        "Save baseline performance numbers",
        "Compare against NVIDIA published baselines",
        "A100 8-GPU all-reduce: ~220-250 GB/s expected",
        "H100 8-GPU all-reduce: ~400-450 GB/s expected"
      ],
      "estimatedDuration": 7,
      "documentationLinks": [
        {
          "title": "DGX Performance Guide",
          "url": "https://docs.nvidia.com/dgx/"
        }
      ]
    }
  ],
  "successCriteria": [
    "Understood NCCL collective operations",
    "Ran baseline NCCL performance tests",
    "Analyzed NVLink vs PCIe performance",
    "Tuned NCCL environment variables",
    "Configured multi-node NCCL",
    "Diagnosed performance issues",
    "Documented optimal configuration"
  ],
  "estimatedTime": 55,
  "prerequisites": [
    "domain4-nccl-test"
  ],
  "tags": [
    "nccl",
    "bandwidth",
    "tuning",
    "performance",
    "collective",
    "distributed",
    "advanced",
    "domain4",
    "exam-critical"
  ]
}
